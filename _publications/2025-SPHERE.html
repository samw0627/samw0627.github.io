---
layout: publication
authors:
  - Xiaohang Tang
  - Sam Wong
  - Marcus Huynh
  - Zicheng He
  - Yalong Yang
  - Yan Chen
description: Effective personalized feedback is crucial for learning programming. However, providing personalized, real-time feedback in large programming classrooms poses significant challenges for instructors. This paper introduces SPHERE, an interactive system that leverages Large Language Models (LLMs) and structured LLM output review to scale personalized feedback for in-class coding activities. SPHERE employs two key components: an Issue Recommendation Component that identifies critical patterns in students' code and discussion, and a Feedback Review Component that uses a ``strategy-detail-verify'' approach for efficient feedback creation and verification. An in-lab, between-subject study demonstrates SPHERE's effectiveness in improving feedback quality and the overall feedback review process compared to a baseline system using off-the-shelf LLM outputs. This work contributes a novel approach to scaling personalized feedback in programming education, addressing the challenges of real-time response, issue prioritization, and large-scale personalization.

link: https://arxiv.org/abs/2410.16513
pdf: https://arxiv.org/pdf/2410.16513
tags:
  - Visualization
  - Learning Analytics
title: "SPHERE: Scaling Personalized Feedback in Programming Classrooms with Structured Review of LLM Outputs"
type:
  - Conference
  - Late Breaking Work
venue: CHI
venue_location: "Yokihama, Japan"
venue_tags:
  - CHI
venue_url: https://chi2025.acm.org/
year: 2025
highlight: true
---

This paper introduces SPHERE, a system that enables instructors
to effectively create and review personalized feedback for in-class
coding activities. Comprehensive personalized feedback is crucial
for programming learning. However, providing such feedback in
large programming classrooms poses significant challenges for instructors. While Large Language Models (LLMs) offer potential
assistance, how to efficiently ensure the quality of LLM-generated
feedback remains an open question. SPHERE guides instructors’ attention to critical students’ issues, empowers them with guided control over LLM-generated feedback, and provides visual scaffolding
to facilitate verification of feedback quality. Our between-subject
study with 20 participants demonstrates SPHERE’s effectiveness
in creating more high-quality feedback while not increasing the
time spent on the overall review process compared to a baseline
system. This work contributes a synergistic approach to scaling
personalized feedback in programming education, addressing the
challenges of real-time response, issue prioritization, and largescale personalization.
